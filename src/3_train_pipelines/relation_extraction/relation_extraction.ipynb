{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8bcaa240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Callable\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "from thinc.types import Floats2d, Ints1d, Ragged, cast\n",
    "from thinc.api import Model, Linear, chain, Logistic\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ee67a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global variable for length of a spacy word vector\n",
    "VECTOR_LENGTH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb689a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "@spacy.registry.architectures.register('microsp_host_rel_model.v1')\n",
    "def create_relation_model(\n",
    "    create_instance_tensor: Model[List[Doc], Floats2d],\n",
    "    classification_layer: Model[Floats2d, Floats2d]\n",
    ") -> Model[List[Doc], Floats2d]:\n",
    "    model = chain(create_instance_tensor, classification_layer)\n",
    "    model.attrs['get_instances'] = create_instance_tensor.attrs['get_instances']\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e72cab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@spacy.registry.architectures.register('microsp_host_rel_classification_layer.v1')\n",
    "def create_classification_layer(\n",
    "    n0: int = None, nI: int = None\n",
    ") -> Model[Floats2d, Floats2d]:\n",
    "    return chain(Linear(n0=n0, nI=nI), Logistic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a62e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_forward(\n",
    "    model: Model[List[Doc], Floats2d],\n",
    "    docs: List[Doc],\n",
    "    is_train: bool\n",
    ") -> Tuple[Floats2d, Floats2d]:\n",
    "    tok2vec = model.get_ref('tok2vec')\n",
    "    pooling = model.get_ref('pooling')  # default pool = mean pool\n",
    "    get_instances = model.attrs['get_instances']\n",
    "    \n",
    "    all_instances = [get_instances(doc) for doc in docs]\n",
    "    tokvecs, bp_tokvects = tok2vec(docs, is_train)  # tok2vec is trained\n",
    "    \n",
    "    # for making instance vectors, then join vertically into matrix\n",
    "    ents = []\n",
    "    lengths = []\n",
    "    \n",
    "    for doc_nr, (instances, tokvec) in enumerate(zip(all_instances, tokvecs)):\n",
    "        token_indices = []\n",
    "        for instance in instances:\n",
    "            for ent in instance:\n",
    "                token_indices.extend([i for i in range(ent.start, ent.end)])\n",
    "                lengths.append(ent.end - ent.start)\n",
    "            \n",
    "            ents.append(tokvec[token_indices])\n",
    "            \n",
    "        lengths = cast(Ints1d, model.ops.asarray(lengths, dtype='int32'))\n",
    "        entities = Ragged(model.ops.flatten(ents), lengths)\n",
    "        pooled, bp_pooled = pooling(entities, is_train)\n",
    "        \n",
    "        # Reshape so that pairs of rows are concatenated\n",
    "        relations = model.ops.reshape2f(pooled, -1, pooled.shape[1] + 2)\n",
    "        \n",
    "        def backprop(d_relations: Floats2d) -> List[Doc]:\n",
    "            d_pooled = model.ops.reshape2f(d_relations, d_relations.shape[0] * 2, -1)\n",
    "            d_ents = bp_pooled(d_pooled).data\n",
    "            d_tokvecs = []\n",
    "            ent_index = 0\n",
    "            for doc_nr, instances in enumerate(all_instances):\n",
    "                shape = tokvecs[doc_nr].shape\n",
    "                d_tokvec = model.ops.alloc2f(*shape)\n",
    "                count_occ = model.ops.alloc2f(*shape)\n",
    "                for instance in instances:\n",
    "                    for ent in instance:\n",
    "                        d_tokvec[ent.start : ent.end] += d_ents[ent_index]\n",
    "                        count_occ[ent.start : ent.end] += 1\n",
    "                        ent_index += ent.end - ent.start\n",
    "                d_tokvec /= count_occ + 0.00000000001\n",
    "                d_tokvecs.append(d_tokvec)\n",
    "\n",
    "            d_docs = bp_tokvecs(d_tokvecs)\n",
    "            return d_docs\n",
    "\n",
    "    return relations, backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "dd705654",
   "metadata": {},
   "outputs": [],
   "source": [
    "@spacy.registry.misc(\"microsp_host_rel_instance_generator.v1\")\n",
    "def create_instances() -> Callable[[Doc], List[np.ndarray]]:\n",
    "    def get_instances(doc: Doc) -> List[np.ndarray]:\n",
    "        def get_abbreviated_name(species: str) -> str:\n",
    "            if ' ' not in species:\n",
    "                return species\n",
    "            \n",
    "            # remove any trailing parenthesized text from species names (ex: names of discovering scientists),\n",
    "            # as these are typically irrelevant\n",
    "            species = re.sub(r' ?\\(.+\\)$', '', species).split()\n",
    "            return f\"{' '.join([s[0] + '.' for s in species[:len(species) - 1]])} {species[-1]}\"\n",
    "        \n",
    "        def are_aliased_names(ent_1: str, ent_2: str) -> bool:\n",
    "            # entity names are substrings, identical to each other, or one name is the abbreviation for\n",
    "            # the other\n",
    "            #\n",
    "            # note to self: maybe move second condition to another if statement\n",
    "            abbreviated_name_1 = get_abbreviated_name(ent_1).strip()\n",
    "            abbreviated_name_2 = get_abbreviated_name(ent_2).strip()\n",
    "            if (ent_1 in ent_2 or ent_2 in ent_1) or \\\n",
    "                abbreviated_name_1 in abbreviated_name_2 or \\\n",
    "                abbreviated_name_2 in abbreviated_name_1:\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "        \n",
    "        # in case a microsporidia/host entity spans multiple tokens, take the mean word vector\n",
    "        # ent.as_doc().vector\n",
    "        microsp = [ent for ent in doc.ents if ent.label_ == 'MICROSPORIDIA']\n",
    "        hosts = [ent for ent in doc.ents if ent.label_ == 'HOST']\n",
    "\n",
    "        # keep track of which microsporidia / host names are synonymous to each other\n",
    "        # i.e: abbreviations, same species mentioned >1 time\n",
    "        # ex: 'Amblyospora hasseri' = 'A. hasseri' = 'Amblyospora hasseri (1)' (if all are same entity types)\n",
    "        microsp_aliases = []\n",
    "        host_aliases = []\n",
    "        ents_assigned_aliases = []\n",
    "        \n",
    "        for m in microsp:\n",
    "            if m not in ents_assigned_aliases:\n",
    "                ents_assigned_aliases.append(m)\n",
    "                # see are_aliased_names function for definition of aliased species names\n",
    "                aliases = [m_ for m_ in microsp if m_ not in ents_assigned_aliases and are_aliased_names(m.text, m_.text)]\n",
    "    \n",
    "                if aliases:\n",
    "                    # pool all entity vectors then take average of all aliased species entitiy vectors, for the\n",
    "                    # vector representation for a particular microsporidia\n",
    "                    microsp_aliases.append(np.mean(np.vstack([m.vector] + [a.vector for a in aliases]), axis=0))\n",
    "                    \n",
    "                    # add all aliased entities to list of entities that we've already seen\n",
    "                    ents_assigned_aliases.extend(aliases)\n",
    "                else:\n",
    "                    # if no aliased names for this species, just take the pooled vector for the entity\n",
    "                    microsp_aliases.append(m.vector)\n",
    "        \n",
    "        for h in hosts:\n",
    "            if h not in ents_assigned_aliases:\n",
    "                ents_assigned_aliases.append(h)\n",
    "                aliases = [h_ for h_ in hosts if h_ not in ents_assigned_aliases and are_aliased_names(h.text, h_.text)]\n",
    "    \n",
    "                if aliases:\n",
    "                    ents_assigned_aliases.extend(aliases)\n",
    "                    host_aliases.append(np.mean(np.vstack([h.vector] + [a.vector for a in aliases]), axis=0))\n",
    "                else:\n",
    "                    host_aliases.append(h.vector)\n",
    "        \n",
    "        # 1 instance = average embedding for a pair of microsporidia and host pooled vectors\n",
    "        return [(arr[0] + arr[1]) / 2 for arr in product(microsp_aliases, host_aliases)]\n",
    "        \n",
    "    return get_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86926728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
